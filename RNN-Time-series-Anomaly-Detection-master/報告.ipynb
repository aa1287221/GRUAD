{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db513ab",
   "metadata": {},
   "source": [
    "# PART I. Train Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380cd01",
   "metadata": {},
   "source": [
    "### 導入Tools & Args\n",
    "模型的各項參數從Args裡面做調整設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import preprocess_data\n",
    "from model import model\n",
    "from torch import optim\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "from anomalyDetector import fit_norm_distribution_param\n",
    "import numpy as np\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch RNN Prediction Model on Time-series Dataset')\n",
    "parser.add_argument('--data', type=str, default='ofdm',\n",
    "                    help='type of the dataset (ecg, gesture, power_demand, space_shuttle, respiration, nyc_taxi, ofdm')\n",
    "parser.add_argument('--filename', type=str, default='NoiseSymbol.pkl',\n",
    "                    help='filename of the dataset')\n",
    "parser.add_argument('--model', type=str, default='GRU',\n",
    "                    help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, SRU)')\n",
    "parser.add_argument('--augment', type=bool, default=True,\n",
    "                    help='augment')\n",
    "parser.add_argument('--emsize', type=int, default=32,\n",
    "                    help='size of rnn input features')\n",
    "parser.add_argument('--nhid', type=int, default=32,\n",
    "                    help='number of hidden units per layer')\n",
    "parser.add_argument('--nlayers', type=int, default=2,\n",
    "                    help='number of layers')\n",
    "parser.add_argument('--res_connection', action='store_true',\n",
    "                    help='residual connection')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, #0.0002\n",
    "                    help='initial learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-4,\n",
    "                    help='weight decay')\n",
    "parser.add_argument('--clip', type=float, default=1,\n",
    "                    help='gradient clipping')\n",
    "parser.add_argument('--epochs', type=int, default=20000,\n",
    "                    help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=64, metavar='N',\n",
    "                    help='batch size')\n",
    "parser.add_argument('--eval_batch_size', type=int, default=64, metavar='N',\n",
    "                    help='eval_batch size')\n",
    "parser.add_argument('--bptt', type=int, default=10,\n",
    "                    help='sequence length')\n",
    "parser.add_argument('--teacher_forcing_ratio', type=float, default=0.7,\n",
    "                    help='teacher forcing ratio (deprecated)')\n",
    "parser.add_argument('--dropout', type=float, default=0.2,\n",
    "                    help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--tied', action='store_true',\n",
    "                    help='tie the word embedding and softmax weights (deprecated)')\n",
    "parser.add_argument('--seed', type=int, default=1111,\n",
    "                    help='random seed')\n",
    "parser.add_argument('--device', type=str, default='cuda',\n",
    "                    help='cuda or cpu')\n",
    "parser.add_argument('--log_interval', type=int, default=10, metavar='N',\n",
    "                    help='report interval')\n",
    "parser.add_argument('--save_interval', type=int, default=10, metavar='N',\n",
    "                    help='save interval')\n",
    "parser.add_argument('--save_fig','-s', action='store_true',\n",
    "                    help='save figure')\n",
    "parser.add_argument('--resume','-r',\n",
    "                    help='use checkpoint model parameters as initial parameters (default: False)',\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('--pretrained','-p',\n",
    "                    help='use checkpoint model parameters and do not train anymore (default: False)',\n",
    "                    action=\"store_true\")\n",
    "parser.add_argument('--prediction_window_size', type=int, default=10,\n",
    "                    help='prediction_window_size')\n",
    "args = parser.parse_args()\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb4171",
   "metadata": {},
   "source": [
    "### 讀取Data & 建構Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cf2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeseriesData = preprocess_data.PickleDataLoad(data_type=args.data, filename=args.filename,\n",
    "                                                augment_test_data=args.augment)\n",
    "train_dataset = TimeseriesData.batchify(args,TimeseriesData.trainData, args.batch_size)\n",
    "test_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, args.eval_batch_size)\n",
    "gen_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, 1)\n",
    "\n",
    "\n",
    "feature_dim = TimeseriesData.trainData.size(1)\n",
    "model = model.RNNPredictor(rnn_type = args.model,\n",
    "                           enc_inp_size=feature_dim,\n",
    "                           rnn_inp_size = args.emsize,\n",
    "                           rnn_hid_size = args.nhid,\n",
    "                           dec_out_size=feature_dim,\n",
    "                           nlayers = args.nlayers,\n",
    "                           dropout = args.dropout,\n",
    "                           tie_weights= args.tied,\n",
    "                           res_connection=args.res_connection).to(args.device)\n",
    "optimizer = optim.Adam(model.parameters(), lr= args.lr,weight_decay=args.weight_decay)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a02fa",
   "metadata": {},
   "source": [
    "### 取得Batch size ＆ 生成Testing結果圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804a8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(args,source, i):\n",
    "    seq_len = min(args.bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len] # [ seq_len * batch_size * feature_size ]\n",
    "    target = source[i+1:i+1+seq_len] # [ (seq_len x batch_size x feature_size) ]\n",
    "    return data, target\n",
    "\n",
    "def generate_output(args,epoch, model, gen_dataset, disp_uncertainty=True,startPoint=500, endPoint=3500):\n",
    "    if args.save_fig:\n",
    "        # Turn on evaluation mode which disables dropout.\n",
    "        model.eval()\n",
    "        hidden = model.init_hidden(1)\n",
    "        outSeq = []\n",
    "        upperlim95 = []\n",
    "        lowerlim95 = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(endPoint):\n",
    "                if i>=startPoint:\n",
    "                    # if disp_uncertainty and epoch > 40:\n",
    "                    #     outs = []\n",
    "                    #     model.train()\n",
    "                    #     for i in range(20):\n",
    "                    #         out_, hidden_ = model.forward(out+0.01*Variable(torch.randn(out.size())).cuda(),hidden,noise=True)\n",
    "                    #         outs.append(out_)\n",
    "                    #     model.eval()\n",
    "                    #     outs = torch.cat(outs,dim=0)\n",
    "                    #     out_mean = torch.mean(outs,dim=0) # [bsz * feature_dim]\n",
    "                    #     out_std = torch.std(outs,dim=0) # [bsz * feature_dim]\n",
    "                    #     upperlim95.append(out_mean + 2.58*out_std/np.sqrt(20))\n",
    "                    #     lowerlim95.append(out_mean - 2.58*out_std/np.sqrt(20))\n",
    "\n",
    "                    out, hidden = model.forward(out, hidden)\n",
    "\n",
    "                    #print(out_mean,out)\n",
    "\n",
    "                else:\n",
    "                    out, hidden = model.forward(gen_dataset[i].unsqueeze(0), hidden)\n",
    "                outSeq.append(out.data.cpu()[0][0].unsqueeze(0))\n",
    "\n",
    "\n",
    "        outSeq = torch.cat(outSeq,dim=0) # [seqLength * feature_dim]\n",
    "\n",
    "        target= preprocess_data.reconstruct(gen_dataset.cpu(), TimeseriesData.mean, TimeseriesData.std)\n",
    "        outSeq = preprocess_data.reconstruct(outSeq, TimeseriesData.mean, TimeseriesData.std)\n",
    "        # if epoch>40:\n",
    "        #     upperlim95 = torch.cat(upperlim95, dim=0)\n",
    "        #     lowerlim95 = torch.cat(lowerlim95, dim=0)\n",
    "        #     upperlim95 = preprocess_data.reconstruct(upperlim95.data.cpu().numpy(),TimeseriesData.mean,TimeseriesData.std)\n",
    "        #     lowerlim95 = preprocess_data.reconstruct(lowerlim95.data.cpu().numpy(),TimeseriesData.mean,TimeseriesData.std)\n",
    "\n",
    "        plt.figure(figsize=(15,5))\n",
    "        for i in range(target.size(-1)):\n",
    "            plt.plot(target[:,:,i].numpy(), label='Target'+str(i),\n",
    "                     color='black', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            plt.plot(range(startPoint), outSeq[:startPoint,i].numpy(), label='1-step predictions for target'+str(i),\n",
    "                     color='green', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "            # if epoch>40:\n",
    "            #     plt.plot(range(startPoint, endPoint), upperlim95[:,i].numpy(), label='upperlim'+str(i),\n",
    "            #              color='skyblue', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "            #     plt.plot(range(startPoint, endPoint), lowerlim95[:,i].numpy(), label='lowerlim'+str(i),\n",
    "            #              color='skyblue', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "            plt.plot(range(startPoint, endPoint), outSeq[startPoint:,i].numpy(), label='Recursive predictions for target'+str(i),\n",
    "                     color='blue', marker='.', linestyle='--', markersize=1.5, linewidth=1)\n",
    "\n",
    "        plt.xlim([startPoint-500, endPoint])\n",
    "        plt.xlabel('Index',fontsize=15)\n",
    "        plt.ylabel('Value',fontsize=15)\n",
    "        plt.title('Time-series Prediction on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.text(startPoint-500+10, target.min(), 'Epoch: '+str(epoch),fontsize=15)\n",
    "        save_dir = Path('result',args.data,args.filename).with_suffix('').joinpath('fig_prediction')\n",
    "        save_dir.mkdir(parents=True,exist_ok=True)\n",
    "        plt.savefig(save_dir.joinpath('fig_epoch'+str(epoch)).with_suffix('.png'))\n",
    "        #plt.show()\n",
    "        plt.close()\n",
    "        return outSeq\n",
    "\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4345421",
   "metadata": {},
   "source": [
    "### Training code \n",
    "如果dropout為0則為evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30e28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, train_dataset,epoch):\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Turn on training mode which enables dropout.\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        start_time = time.time()\n",
    "        hidden = model.init_hidden(args.batch_size)\n",
    "        for batch, i in enumerate(range(0, train_dataset.size(0) - 1, args.bptt)):\n",
    "            inputSeq, targetSeq = get_batch(args,train_dataset, i)\n",
    "            # inputSeq: [ seq_len * batch_size * feature_size ]\n",
    "            # targetSeq: [ seq_len * batch_size * feature_size ]\n",
    "\n",
    "            # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "            # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "            hidden = model.repackage_hidden(hidden)\n",
    "            hidden_ = model.repackage_hidden(hidden)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            '''Loss1: Free running loss'''\n",
    "            outVal = inputSeq[0].unsqueeze(0)\n",
    "            outVals=[]\n",
    "            hids1 = []\n",
    "            for i in range(inputSeq.size(0)):\n",
    "                outVal, hidden_, hid = model.forward(outVal, hidden_,return_hiddens=True)\n",
    "                outVals.append(outVal)\n",
    "                hids1.append(hid)\n",
    "            outSeq1 = torch.cat(outVals,dim=0)\n",
    "            hids1 = torch.cat(hids1,dim=0)\n",
    "            loss1 = criterion(outSeq1.contiguous().view(args.batch_size,-1), targetSeq.contiguous().view(args.batch_size,-1))\n",
    "\n",
    "            '''Loss2: Teacher forcing loss'''\n",
    "            outSeq2, hidden, hids2 = model.forward(inputSeq, hidden, return_hiddens=True)\n",
    "            loss2 = criterion(outSeq2.contiguous().view(args.batch_size, -1), targetSeq.contiguous().view(args.batch_size, -1))\n",
    "\n",
    "            '''Loss3: Simplified Professor forcing loss'''\n",
    "            loss3 = criterion(hids1.contiguous().view(args.batch_size,-1), hids2.contiguous().view(args.batch_size,-1).detach())\n",
    "\n",
    "            '''Total loss = Loss1+Loss2+Loss3'''\n",
    "            loss = loss1+loss2+loss3\n",
    "            loss.backward()\n",
    "\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if batch % args.log_interval == 0 and batch > 0:\n",
    "                cur_loss = total_loss / args.log_interval\n",
    "                elapsed = time.time() - start_time\n",
    "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.4f} | '\n",
    "                      'loss {:5.2f} '.format(\n",
    "                    epoch, batch, len(train_dataset) // args.bptt,\n",
    "                                  elapsed * 1000 / args.log_interval, cur_loss))\n",
    "                total_loss = 0\n",
    "                start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319a17e",
   "metadata": {},
   "source": [
    "### Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a613c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, test_dataset):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        hidden = model.init_hidden(args.eval_batch_size)\n",
    "        nbatch = 1\n",
    "        for nbatch, i in enumerate(range(0, test_dataset.size(0) - 1, args.bptt)):\n",
    "            inputSeq, targetSeq = get_batch(args,test_dataset, i)\n",
    "            # inputSeq: [ seq_len * batch_size * feature_size ]\n",
    "            # targetSeq: [ seq_len * batch_size * feature_size ]\n",
    "            hidden_ = model.repackage_hidden(hidden)\n",
    "            '''Loss1: Free running loss'''\n",
    "            outVal = inputSeq[0].unsqueeze(0)\n",
    "            outVals=[]\n",
    "            hids1 = []\n",
    "            for i in range(inputSeq.size(0)):\n",
    "                outVal, hidden_, hid = model.forward(outVal, hidden_,return_hiddens=True)\n",
    "                outVals.append(outVal)\n",
    "                hids1.append(hid)\n",
    "            outSeq1 = torch.cat(outVals,dim=0)\n",
    "            hids1 = torch.cat(hids1,dim=0)\n",
    "            loss1 = criterion(outSeq1.contiguous().view(args.batch_size,-1), targetSeq.contiguous().view(args.batch_size,-1))\n",
    "\n",
    "            '''Loss2: Teacher forcing loss'''\n",
    "            outSeq2, hidden, hids2 = model.forward(inputSeq, hidden, return_hiddens=True)\n",
    "            loss2 = criterion(outSeq2.contiguous().view(args.batch_size, -1), targetSeq.contiguous().view(args.batch_size, -1))\n",
    "\n",
    "            '''Loss3: Simplified Professor forcing loss'''\n",
    "            loss3 = criterion(hids1.view(args.batch_size,-1), hids2.view(args.batch_size,-1).detach())\n",
    "\n",
    "            '''Total loss = Loss1+Loss2+Loss3'''\n",
    "            loss = loss1+loss2+loss3\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (nbatch+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c8fc7",
   "metadata": {},
   "source": [
    "### Main code\n",
    "每當training十次epoch後，儲存一次checkpoint，讓我們可以提早結束training，\n",
    "\n",
    "如需繼續training可以從checkpoint處繼續，或是透過checkpoint將pretrained的model讀取出來進行參數調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef297322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over epochs.\n",
    "if args.resume or args.pretrained:\n",
    "    print(\"=> loading checkpoint \")\n",
    "    checkpoint = torch.load(Path('save', args.data, 'checkpoint', args.filename).with_suffix('.pth'))\n",
    "    args, start_epoch, best_val_loss = model.load_checkpoint(args,checkpoint,feature_dim)\n",
    "    optimizer.load_state_dict((checkpoint['optimizer']))\n",
    "    del checkpoint\n",
    "    epoch = start_epoch\n",
    "    print(\"=> loaded checkpoint\")\n",
    "else:\n",
    "    epoch = 1\n",
    "    start_epoch = 1\n",
    "    best_val_loss = float('inf')\n",
    "    print(\"=> Start training from scratch\")\n",
    "print('-' * 89)\n",
    "print(args)\n",
    "print('-' * 89)\n",
    "\n",
    "if not args.pretrained:\n",
    "    # At any point you can hit Ctrl + C to break out of training early.\n",
    "    try:\n",
    "        for epoch in range(start_epoch, args.epochs+1):\n",
    "\n",
    "            epoch_start_time = time.time()\n",
    "            train(args,model,train_dataset,epoch)\n",
    "            val_loss = evaluate(args,model,test_dataset)\n",
    "            print('-' * 89)\n",
    "            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.4f} | '.format(epoch, (time.time() - epoch_start_time), val_loss))\n",
    "            print('-' * 89)\n",
    "\n",
    "            generate_output(args,epoch,model,gen_dataset,startPoint=1500)\n",
    "\n",
    "            if epoch%args.save_interval==0:\n",
    "                # Save the model if the validation loss is the best we've seen so far.\n",
    "                is_best = val_loss < best_val_loss\n",
    "                best_val_loss = min(val_loss, best_val_loss)\n",
    "                model_dictionary = {'epoch': epoch,\n",
    "                                    'best_loss': best_val_loss,\n",
    "                                    'state_dict': model.state_dict(),\n",
    "                                    'optimizer': optimizer.state_dict(),\n",
    "                                    'args':args\n",
    "                                    }\n",
    "                model.save_checkpoint(model_dictionary, is_best)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('-' * 89)\n",
    "        print('Exiting from training early')\n",
    "\n",
    "\n",
    "# Calculate mean and covariance for each channel's prediction errors, and save them with the trained model\n",
    "print('=> calculating mean and covariance')\n",
    "means, covs = list(),list()\n",
    "train_dataset = TimeseriesData.batchify(args, TimeseriesData.trainData, bsz=1)\n",
    "for channel_idx in range(model.enc_input_size):\n",
    "    mean, cov = fit_norm_distribution_param(args,model,train_dataset[:TimeseriesData.length],channel_idx)\n",
    "    means.append(mean), covs.append(cov)\n",
    "model_dictionary = {'epoch': max(epoch,start_epoch),\n",
    "                    'best_loss': best_val_loss,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'args': args,\n",
    "                    'means': means,\n",
    "                    'covs': covs\n",
    "                    }\n",
    "model.save_checkpoint(model_dictionary, True)\n",
    "print('-' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15befb43",
   "metadata": {},
   "source": [
    "# PART II. Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a476ae6e",
   "metadata": {},
   "source": [
    "### 導入Tools & Args\n",
    "模型的各項參數從Args裡面做調整設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05419029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import pickle\n",
    "import preprocess_data\n",
    "from model import model\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from anomalyDetector import fit_norm_distribution_param\n",
    "from anomalyDetector import anomalyScore\n",
    "from anomalyDetector import get_precision_recall\n",
    "parser = argparse.ArgumentParser(description='PyTorch RNN Anomaly Detection Model')\n",
    "parser.add_argument('--prediction_window_size', type=int, default=10,\n",
    "                    help='prediction_window_size')\n",
    "parser.add_argument('--data', type=str, default='ofdm',\n",
    "                    help='type of the dataset (ecg, gesture, power_demand, space_shuttle, respiration, nyc_taxi, ofdm')\n",
    "parser.add_argument('--filename', type=str, default='NoiseSymbol.pkl',\n",
    "                    help='filename of the dataset')\n",
    "parser.add_argument('--save_fig','-s', action='store_true',\n",
    "                    help='save results as figures')\n",
    "parser.add_argument('--compensate', action='store_true',\n",
    "                    help='compensate anomaly score using anomaly score esimation')\n",
    "parser.add_argument('--beta', type=float, default=1.0,\n",
    "                    help='beta value for f-beta score')\n",
    "\n",
    "\n",
    "args_ = parser.parse_args()\n",
    "print('-' * 89)\n",
    "print(\"=> loading checkpoint \")\n",
    "checkpoint = torch.load(str(Path('save',args_.data,'checkpoint',args_.filename).with_suffix('.pth')))\n",
    "args = checkpoint['args']\n",
    "args.prediction_window_size= args_.prediction_window_size\n",
    "args.beta = args_.beta\n",
    "args.save_fig = args_.save_fig\n",
    "args.compensate = args_.compensate\n",
    "print(\"=> loaded checkpoint\")\n",
    "\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af18ed3",
   "metadata": {},
   "source": [
    "### 讀取Data & 建構Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da129c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TimeseriesData = preprocess_data.PickleDataLoad(data_type=args.data,filename=args.filename, augment_test_data=False)\n",
    "train_dataset = TimeseriesData.batchify(args,TimeseriesData.trainData[:TimeseriesData.length], bsz=1)\n",
    "test_dataset = TimeseriesData.batchify(args,TimeseriesData.testData, bsz=1)\n",
    "\n",
    "\n",
    "nfeatures = TimeseriesData.trainData.size(-1)\n",
    "model = model.RNNPredictor(rnn_type = args.model,\n",
    "                           enc_inp_size=nfeatures,\n",
    "                           rnn_inp_size = args.emsize,\n",
    "                           rnn_hid_size = args.nhid,\n",
    "                           dec_out_size=nfeatures,\n",
    "                           nlayers = args.nlayers,\n",
    "                           res_connection=args.res_connection).to(args.device)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0537c2",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76451b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, predicted_scores, precisions, recalls, f_betas = list(), list(), list(), list(), list()\n",
    "targets, mean_predictions, oneStep_predictions, Nstep_predictions = list(), list(), list(), list()\n",
    "try:\n",
    "    # For each channel in the dataset\n",
    "    for channel_idx in range(nfeatures):\n",
    "        ''' 1. Load mean and covariance if they are pre-calculated, if not calculate them. '''\n",
    "        # Mean and covariance are calculated on train dataset.\n",
    "        if 'means' in checkpoint.keys() and 'covs' in checkpoint.keys():\n",
    "            print('=> loading pre-calculated mean and covariance')\n",
    "            mean, cov = checkpoint['means'][channel_idx], checkpoint['covs'][channel_idx]\n",
    "        else:\n",
    "            print('=> calculating mean and covariance')\n",
    "            mean, cov = fit_norm_distribution_param(args, model, train_dataset, channel_idx=channel_idx)\n",
    "\n",
    "        ''' 2. Train anomaly score predictor using support vector regression (SVR). (Optional) '''\n",
    "        # An anomaly score predictor is trained\n",
    "        # given hidden layer output and the corresponding anomaly score on train dataset.\n",
    "        # Predicted anomaly scores on test dataset can be used for the baseline of the adaptive threshold.\n",
    "        if args.compensate:\n",
    "            print('=> training an SVR as anomaly score predictor')\n",
    "            train_score, _, _, hiddens, _ = anomalyScore(args, model, train_dataset, mean, cov, channel_idx=channel_idx)\n",
    "            score_predictor = GridSearchCV(SVR(), cv=5,param_grid={\"C\": [1e0, 1e1, 1e2],\"gamma\": np.logspace(-1, 1, 3)})\n",
    "            score_predictor.fit(torch.cat(hiddens,dim=0).numpy(), train_score.cpu().numpy())\n",
    "        else:\n",
    "            score_predictor=None\n",
    "\n",
    "        ''' 3. Calculate anomaly scores'''\n",
    "        # Anomaly scores are calculated on the test dataset\n",
    "        # given the mean and the covariance calculated on the train dataset\n",
    "        print('=> calculating anomaly scores')\n",
    "        score, sorted_prediction, sorted_error, _, predicted_score = anomalyScore(args, model, test_dataset, mean, cov,\n",
    "                                                                                  score_predictor=score_predictor,\n",
    "                                                                                  channel_idx=channel_idx)\n",
    "\n",
    "        ''' 4. Evaluate the result '''\n",
    "        # The obtained anomaly scores are evaluated by measuring precision, recall, and f_beta scores\n",
    "        # The precision, recall, f_beta scores are are calculated repeatedly,\n",
    "        # sampling the threshold from 1 to the maximum anomaly score value, either equidistantly or logarithmically.\n",
    "        print('=> calculating precision, recall, and f_beta')\n",
    "        precision, recall, f_beta = get_precision_recall(args, score, num_samples=1000, beta=args.beta,\n",
    "                                                         label=TimeseriesData.testLabel.to(args.device))\n",
    "        print('data: ',args.data,' filename: ',args.filename,\n",
    "              ' f-beta (no compensation): ', f_beta.max().item(),' beta: ',args.beta)\n",
    "        if args.compensate:\n",
    "            precision, recall, f_beta = get_precision_recall(args, score, num_samples=1000, beta=args.beta,\n",
    "                                                             label=TimeseriesData.testLabel.to(args.device),\n",
    "                                                             predicted_score=predicted_score)\n",
    "            print('data: ',args.data,' filename: ',args.filename,\n",
    "                  ' f-beta    (compensation): ', f_beta.max().item(),' beta: ',args.beta)\n",
    "\n",
    "\n",
    "        target = preprocess_data.reconstruct(test_dataset.cpu()[:, 0, channel_idx],\n",
    "                                             TimeseriesData.mean[channel_idx],\n",
    "                                             TimeseriesData.std[channel_idx]).numpy()\n",
    "        mean_prediction = preprocess_data.reconstruct(sorted_prediction.mean(dim=1).cpu(),\n",
    "                                                      TimeseriesData.mean[channel_idx],\n",
    "                                                      TimeseriesData.std[channel_idx]).numpy()\n",
    "        oneStep_prediction = preprocess_data.reconstruct(sorted_prediction[:, -1].cpu(),\n",
    "                                                         TimeseriesData.mean[channel_idx],\n",
    "                                                         TimeseriesData.std[channel_idx]).numpy()\n",
    "        Nstep_prediction = preprocess_data.reconstruct(sorted_prediction[:, 0].cpu(),\n",
    "                                                       TimeseriesData.mean[channel_idx],\n",
    "                                                       TimeseriesData.std[channel_idx]).numpy()\n",
    "        sorted_errors_mean = sorted_error.abs().mean(dim=1).cpu()\n",
    "        sorted_errors_mean *= TimeseriesData.std[channel_idx]\n",
    "        sorted_errors_mean = sorted_errors_mean.numpy()\n",
    "        score = score.cpu()\n",
    "        scores.append(score), targets.append(target), predicted_scores.append(predicted_score)\n",
    "        mean_predictions.append(mean_prediction), oneStep_predictions.append(oneStep_prediction)\n",
    "        Nstep_predictions.append(Nstep_prediction)\n",
    "        precisions.append(precision), recalls.append(recall), f_betas.append(f_beta)\n",
    "\n",
    "\n",
    "        if args.save_fig:\n",
    "            save_dir = Path('result',args.data,args.filename).with_suffix('').joinpath('fig_detection')\n",
    "            save_dir.mkdir(parents=True,exist_ok=True)\n",
    "            plt.plot(precision.cpu().numpy(),label='precision')\n",
    "            plt.plot(recall.cpu().numpy(),label='recall')\n",
    "            plt.plot(f_beta.cpu().numpy(), label='f1')\n",
    "            plt.legend()\n",
    "            plt.xlabel('Threshold (log scale)')\n",
    "            plt.ylabel('Value')\n",
    "            plt.title('Anomaly Detection on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "            plt.savefig(str(save_dir.joinpath('fig_f_beta_channel'+str(channel_idx)).with_suffix('.png')))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "            fig, ax1 = plt.subplots(figsize=(15,5))\n",
    "            ax1.plot(target,label='Target',\n",
    "                     color='black',  marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(mean_prediction, label='Mean predictions',\n",
    "                     color='purple', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(oneStep_prediction, label='1-step predictions',\n",
    "                     color='green', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(Nstep_prediction, label=str(args.prediction_window_size) + '-step predictions',\n",
    "                     color='blue', marker='.', linestyle='--', markersize=1, linewidth=0.5)\n",
    "            ax1.plot(sorted_errors_mean,label='Absolute mean prediction errors',\n",
    "                     color='orange', marker='.', linestyle='--', markersize=1, linewidth=1.0)\n",
    "            ax1.legend(loc='upper left')\n",
    "            ax1.set_ylabel('Value',fontsize=15)\n",
    "            ax1.set_xlabel('Index',fontsize=15)\n",
    "            ax2 = ax1.twinx()\n",
    "            ax2.plot(score.numpy().reshape(-1, 1), label='Anomaly scores from \\nmultivariate normal distribution',\n",
    "                     color='red', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "            if args.compensate:\n",
    "                ax2.plot(predicted_score, label='Predicted anomaly scores from SVR',\n",
    "                         color='cyan', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "            #ax2.plot(score.reshape(-1,1)/(predicted_score+1),label='Anomaly scores from \\nmultivariate normal distribution',\n",
    "            #        color='hotpink', marker='.', linestyle='--', markersize=1, linewidth=1)\n",
    "            ax2.legend(loc='upper right')\n",
    "            ax2.set_ylabel('anomaly score',fontsize=15)\n",
    "            #plt.axvspan(2830,2900 , color='yellow', alpha=0.3)\n",
    "            plt.title('Anomaly Detection on ' + args.data + ' Dataset', fontsize=18, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.xlim([0,len(test_dataset)])\n",
    "            plt.savefig(str(save_dir.joinpath('fig_scores_channel'+str(channel_idx)).with_suffix('.png')))\n",
    "            #plt.show()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "\n",
    "print('=> saving the results as pickle extensions')\n",
    "save_dir = Path('result', args.data, args.filename).with_suffix('')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "pickle.dump(targets, open(str(save_dir.joinpath('target.pkl')),'wb'))\n",
    "pickle.dump(mean_predictions, open(str(save_dir.joinpath('mean_predictions.pkl')),'wb'))\n",
    "pickle.dump(oneStep_predictions, open(str(save_dir.joinpath('oneStep_predictions.pkl')),'wb'))\n",
    "pickle.dump(Nstep_predictions, open(str(save_dir.joinpath('Nstep_predictions.pkl')),'wb'))\n",
    "pickle.dump(scores, open(str(save_dir.joinpath('score.pkl')),'wb'))\n",
    "pickle.dump(predicted_scores, open(str(save_dir.joinpath('predicted_scores.pkl')),'wb'))\n",
    "pickle.dump(precisions, open(str(save_dir.joinpath('precision.pkl')),'wb'))\n",
    "pickle.dump(recalls, open(str(save_dir.joinpath('recall.pkl')),'wb'))\n",
    "pickle.dump(f_betas, open(str(save_dir.joinpath('f_beta.pkl')),'wb'))\n",
    "print('-' * 89)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
